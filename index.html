<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="RETrO: Rendering and Extracting Transparent Objects using TensoRF and Feature Field Distillation">
  <meta name="keywords" content="TensoRF, feature field distillation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RETrO: Rendering and Extracting Transparent Objects using TensoRF and Feature Field Distillation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://www.scenerepresentations.org/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">RETrO: Rendering and Extracting Transparent Objects using TensoRF and Feature Field Distillation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://weimegan.github.io">Megan Wei</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://k8xu.github.io">Kate Xu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/anushka-ray/">Anushka Ray</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Massachusetts Institute of Technology</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/weimegan/TensoRF-Transparent"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Problem statement. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Problem Statement</h2>
        <div class="content has-text-justified">
            <p>
              We present a model that renders and extracts features from 3D scenes that contain transparent objects.
              There are existing datasets of transparent objects and methods to render transparent objects, such as
              KeyPose and Dex-NeRF. However, few approaches render and extract transparent objects from scenes. As a
              result, we propose a method that involves training TensoRF instead of NeRF on a dataset of transparent
              objects for faster scene rendering. We employ feature field distillation to relate regions of a scene
              with their features, so that we can render specific features within a scene. In addition to
              qualitatively evaluating the renderings of the transparent objects, we will use quantitative metrics
              such as Peak Signal to Noise Ratio (PSNR) and mean squared error (MSE) loss to evaluate our model
              performance. Through this work, we hope to test the limits of neural rendering models and expand upon
              the types of objects that these algorithms can reconstruct.
            </p>
          </div>
      </div>
    </div>
    <!--/ Problem statement. -->

    <!-- Related work. -->
    <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
            <h2 class="title is-3">Related Work</h2>
            <div class ="content has-text-justified">
                <p> 
                  We will extend the work of <a href="https://arxiv.org/abs/2205.15585">“Decomposing NeRF for Editing via Feature Field Distillation”</a>. 
                  To mitigate the limitations present in other works, the paper introduces Distilled Feature Fields (DFFs) to locally edit scenes rendered by NeRF. 
                  DFFs map a coordinate <span class="bolded">x</span> and a viewing direction <span class="bolded">d</span> to a density, color, and feature. 
                  The way DFFs are trained is by minimizing the difference between the features rendered by NeRF and features that are predicted by a pre-trained 2D image feature encoder. 
                  In addition, the difference between the rendered and ground-truth pixel colors is also minimized. The pre-trained feature encoders that the paper uses are LSeg and DINO. 
                </p> 

                <p>
                  Without a pre-trained feature extractor, it is difficult to make localized, query-based edits on specific objects in the 3D scene. 
                  This is because scenes rendered by NeRF are implicitly encoded in the weights of an MLP or voxel grid and are not object-centric. 
                  An example of this occurs in <a href="https://arxiv.org/abs/2112.05139">CLIP-NeRF</a>, which is a model that uses CLIP to edit NeRF renderings using text prompts. 
                  CLIP is a neural network with zero-shot performance that learns relationships between visual concepts and natural language. 
                  However, CLIP-NeRF often edits additional regions unrelated to the object specified by the text query. <a href="https://arxiv.org/abs/2012.08503">Some works</a> also 
                  reconstruct scenes with more constrained representations with domain or situation specific pipelines. However, this limits the types of scenes and objects that can be edited. 
                </p>

                <p>
                  Therefore, "Decomposing NeRF for Editing via Feature Field Distillation" uses <a href="https://arxiv.org/abs/2104.14294">DINO</a> and LSeg as feature encoders for their DFF model. 
                  DINO is a model that uses self-supervised learning to semantically segment 2D images. The model works by using self-distillation, a method that uses teacher and student networks. 
                  In the forward pass, the teacher and student networks receive two different crops of the image as an input. The output of the teacher network is centered, and both the teacher and 
                  student network outputs are passed through softmax and loss functions. During backpropagation, only the student's weights are updated. The teacher's weights are updated by applying 
                  an exponential moving average on the student's weights.
                </p> 
                <p> 
                  <a href="https://arxiv.org/abs/2201.03546">LSeg (Language-Driven Semantic Segmentation)</a> is a zero-shot semantic image segmentation model. Traditional semantic segmentation models 
                  require labeled training data which is usually created by human annotators. This process is both time and labor intensive, and it is also prone to human-errors. Some zero-shot 
                  segmentation models use standard word embeddings while focusing on the image encoder. LSeg improves upon zero-shot semantic segmentation by using the CLIP model to train an image 
                  encoder to produce input image embeddings that are similar to the corresponding label embeddings produced by the text encoder.
                </p>
                <p>
                  Although NeRF renderings are used in some of the mentioned papers, it is time-consuming to train and can take up to several days to model a single scene. To mitigate this, 
                  <a href="https://arxiv.org/abs/2203.09517">TensoRF</a> decomposes 4D radiance field tensors into multiple low-rank tensors to enable faster reconstruction and higher quality 
                  renderings than NeRF.
                </p>
                <p>
                  Work on rendering transparent objects is limited. <a href="https://arxiv.org/abs/2110.14217">Dex-NeRF</a>  uses NeRF to render the geometry of transparent objects to improve robot 
                  manipulation. The authors of Dex-NeRF have also created a publicly available dataset of real and synthetic transparent objects in real-world settings. While Dex-NeRF improves 
                  rendering scenes with transparent objects, it does not enable editing of scenes containing transparent objects.
                </p>
            </div>
        </div>
    </div>
    <!--/ Related work. -->

    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
            <h2 class="title is-3">Method</h2>
            <img src="static/images/feature_field.png" alt="Feature and Radiance Fields">
            <p>
              
              <span class="bolded">Feature and Radiance Fields</span>
              The radiance field was trained on the LLFF flower images. Given an input of a sampled 3D point, the radiance field outputs the color and density. 
              The feature field was trained using the DINO feature representation for each image, which was obtained from extracting visual descriptors from <a href="https://github.com/ShirAmir/dino-vit-features">this implementation</a>. 
              Given a sampeld 3D point as input, the feature field outputs the DINO feature at that point.
            </p>
            <br>
            <img src="static/images/2d_scene_decomposition.png" alt="2D Scene Decomposition">
            <p>
              <span class="bolded">2D Scene Decomposition</span>
              During 2D scene decomposition, we select an image patch query of the feature of interest (ex. flower, bottle) and get the DINO feature vector corresponding to the pixels in the image patch. 
              We also have the DINO feature vectors for the entire image. To find the pixels in the image that would match the image patch, we compute the dot product between the patch feature vector and 
              the feature vectors from the image. This results in a list of values corresponding to each pixel in the image. These values would be greater for pixels that are more similar to the patch. 
              To extract the features most similar to the patch, we find a threshold that extracts the desired object from the patch in the image. This was obtained from plotting the distribution of thresholds 
              and querying for thresholds that separate modes of the distribution. We keep the RGB values for the values that are above the threshold. For values below the threshold, we set the RGB values to 0. 
              As a result, the flower in the image is extracted and the background and leaves are set to 0. In the transparent object case, the bottle is extracted. However, some parts of the shadow were extracted 
              because the feature vector was very similar for the bottle and its shadow.
            </p>
            <br>
            <img src="static/images/3d_scene_decomposition.png" alt="3D Scene Decomposition">
            <p>
              <span class="bolded">3D Scene Decomposition</span>
              Similar to 2D scene decomposition, we apply a similar mechanism for 3D scene decomposition. For each sampled point, we apply the feature and radiance fields. The feature field outputs the DINO feature 
              feature at that point, and we compute a dot product between the DINO feature and the patch query feature. In this case, the patch query is the DINO feature representing a section of the flower. The radiance 
              field outputs hte color and density at the sampled point. If the value of the dot product is less than the threshold, we set the density to 0. Otherwise, we keep the density output from the radiance field as is. 
              As a result, only the areas corresponding to the feature in the selected patch will be rendered. In this casem the flower.
            </p>
        </div>
    </div>
    <!--/ Method. -->

    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
            <h2 class="title is-3">Results</h2>

            <h2 class="subtitle is-3">Rendering 3D Flower Scene (Opaque)</h2>
            <video autoplay="true" class="video" loop="true" muted>
                <source src="static/videos/video_default.mp4">
            </video>
            <p>
                As an initial experiment, we use the flower images from the LLFF dataset to render the scene using vanilla TensoRF. No changes
                were made to the TensoRF codebase to generate this result. 
            </p>
            
            <h2 class="subtitle is-3">Extracting Features from 2D Flower Image (Opaque)</h2>
            <img src="static/images/flower_extract_2d_image.png" alt="Image for Extracting Features from 2D Flower">
            <img src="static/images/flower_extract_2d_hist.png" alt="Histogram for Extracting Features from 2D Flower">
            <p>
                In order to extract objects from the 2D flower image, we used the DINO feature extractor to obtain the features
                at the 11th layer of the model. We extract the features corresponding to the flower in the image by calculating a 
                dot product between a known flower patch feature and all features in the image. Using the histogram of dot product results,
                we select a threshold for values that correspond to the flower. We choose a threshold of 10 for the flower and change the pixels
                whose dot products are lower than the threshold to be black.
            </p>

            <h2 class="subtitle is-3">Extracting and Rendering Features from 3D Flower Scene (Opaque)</h2>
            <p>
                We attempted to extract and only render the flower features from the scene. Due to computation constraints, we only render a
                limited number of views using our trained feature and radiance fields. We used different thresholds for the dot product between
                the patch flower feature and the DINO features from the feature field. Below are the results from two thresholds. As shown in the
                videos, the threshold of -0.5 removes more of the background features. However, it also removes some unintended pixels
                of the flower in the scene. On the other hand, the threshold of -1 keeps more of the pixels with the flower, but it also does not
                remove the non-flower pixels. We hope to try out additional thresholds in the future to better extract the flower features,
                as well as render the scene using more views.
            </p>

            <video autoplay="true" class="video" loop="true" muted>
                <source src="static/videos/video-5it-05th.mp4">
            </video>
            <p>
                <span class="bolded">Video from attempting to extract and render flower with threshold -0.5</span>
            </p>

            <video autoplay="true" class="video" loop="true" muted>
                <source src="static/videos/video-5it-1th.mp4">
            </video>
            <p>
                <span class="bolded">Video from attempting to extract and render flower with threshold -1</span>
            </p>


            <h2 class="subtitle is-3">Extracting Features from 2D Image with Transparent Object</h2>
            <p>
                We use the same methodology to extract features from the 2D images containing a transparent object as we did in the opaque case. 
                This example image is from the KeyPose dataset, and we use a threshold of 25 to extract the bottle from the image. Although the bottle
                is extracted correctly, the extracted image also includes areas of the bottle's shadow. This may be because the DINO feature vector is 
                very similar for the bottle and its shadow.
            </p>

            <h2 class="subtitle is-3">Rendering Features from 3D Scene with Transparent Object</h2>
            <img src="static/images/coord.png" alt="Camera coordinate systems">
            <p>
                We attempted to extract and render transparent objects from 3D scenes using images from the KeyPose and Dex-NeRF datasets.
                We used the transforms matrix and camera intrinsics provided by both datasets. However, we believe that the poses for
                both of these datasets are in the format of the OpenCV coordinate system. We attempted to convert the rotation matrix for these
                datasets from OpenCV to the LLFF coordinate system because TensoRF converts from LLFF to NeRF coordinate system, as shown
                in the figure above. However, we found that the rendered video output on the transparent object datasets using our
                modified poses looks incorrect.
            </p>

            <video autoplay="true" class="video" loop="true" muted>
                <source src="static/videos/video-keypose-lol.mp4">
            </video>
            <p>
                <span class="bolded">Rendered video output for images from KeyPose dataset</span>
            </p>

            <video autoplay="true" class="video" loop="true" muted>
                <source src="static/videos/video-dexnerf-lol.mp4">
            </video>
            <p>
                <span class="bolded">Rendered video output for images from Dex-NeRF dataset</span>
            </p>
        </div>
    </div>
    <!--/ Results. -->

  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/weimegan/TensoRF-Transparent" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website template credits: <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
