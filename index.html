<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="RETrO: Rendering and Editing Transparent Objects using TensoRF and Feature Field Distillation">
  <meta name="keywords" content="TensoRF, feature field distillation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RETrO: Rendering and Editing Transparent Objects using TensoRF and Feature Field Distillation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://www.scenerepresentations.org/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">RETrO: Rendering and Editing Transparent Objects using TensoRF and Feature Field Distillation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://weimegan.github.io">Megan Wei</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://k8xu.github.io">Kate Xu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/anushka-ray/">Anushka Ray</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Massachusetts Institute of Technology</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present the first method capable of photorealistically reconstructing a non-rigidly
            deforming scene using photos/videos captured casually from mobile phones.
          </p>
          <p>
            Our approach augments neural radiance fields
            (NeRF) by optimizing an
            additional continuous volumetric deformation field that warps each observed point into a
            canonical 5D NeRF.
            We observe that these NeRF-like deformation fields are prone to local minima, and
            propose a coarse-to-fine optimization method for coordinate-based models that allows for
            more robust optimization.
            By adapting principles from geometry processing and physical simulation to NeRF-like
            models, we propose an elastic regularization of the deformation field that further
            improves robustness.
          </p>
          <p>
            We show that <span class="dnerf">Nerfies</span> can turn casually captured selfie
            photos/videos into deformable NeRF
            models that allow for photorealistic renderings of the subject from arbitrary
            viewpoints, which we dub <i>"nerfies"</i>. We evaluate our method by collecting data
            using a
            rig with two mobile phones that take time-synchronized photos, yielding train/validation
            images of the same pose at different viewpoints. We show that our method faithfully
            reconstructs non-rigidly deforming scenes and reproduces unseen views with high
            fidelity.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Problem statement. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Problem Statement</h2>
        <div class="content has-text-justified">
            <p>
              We present a model that renders and extracts features from 3D scenes that contain transparent objects.
              There are existing datasets of transparent objects and methods to render transparent objects, such as
              KeyPose and Dex-NeRF. However, few approaches render and extract transparent objects from scenes. As a
              result, we propose a method that involves training TensoRF instead of NeRF on a dataset of transparent
              objects for faster scene rendering. We employ feature field distillation to relate regions of a scene
              with their features, so that we can render specific features within a scene. In addition to
              qualitatively evaluating the renderings of the transparent objects, we will use quantitative metrics
              such as Peak Signal to Noise Ratio (PSNR) and mean squared error (MSE) loss to evaluate our model
              performance. Through this work, we hope to test the limits of neural rendering models and expand upon
              the types of objects that these algorithms can reconstruct.
            </p>
          </div>
      </div>
    </div>
    <!--/ Problem statement. -->

    <!-- Related work. -->
    <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
            <h2 class="title is-3">Related Work</h2>
            <div class ="content has-text-justified">
                <p> 
                  We will extend the work of <a href="https://arxiv.org/abs/2205.15585">“Decomposing NeRF for Editing via Feature Field Distillation”</a>. To mitigate the limitations present in other works, the paper introduces Distilled Feature Fields (DFFs) to locally edit scenes rendered by NeRF. DFFs map a coordinate <span class="bolded">x</span> and a viewing direction <span class="bolded">d</span>to a density, color, and feature. The way DFFs are trained is by minimizing the difference between the features rendered by NeRF and features that are predicted by a pre-trained 2D image feature encoder. In addition, the difference between the rendered and ground-truth pixel colors is also minimized. The pre-trained feature encoders that the paper uses are LSeg and DINO. 
                </p> 

                <p>
                  Without a pre-trained feature extractor, it is difficult to make localized, query-based edits on specific objects in the 3D scene. This is because scenes rendered by NeRF are implicitly encoded in the weights of an MLP or voxel grid and are not object-centric. An example of this occurs in <a href="https://arxiv.org/abs/2112.05139">CLIP-NeRF</a>, which is a model that uses CLIP to edit NeRF renderings using text prompts. CLIP is a neural network with zero-shot performance that learns relationships between visual concepts and natural language. However, CLIP-NeRF often edits additional regions unrelated to the object specified by the text query. <a href="https://arxiv.org/abs/2012.08503">Some works</a> also reconstruct scenes with more constrained representations with domain or situation specific pipelines. However, this limits the types of scenes and objects that can be edited. 
                </p>

                <p>
                  Therefore, "Decomposing NeRF for Editing via Feature Field Distillation" uses <a href="https://arxiv.org/abs/2104.14294">DINO</a> and LSeg as feature encoders for their DFF model. DINO is a model that uses self-supervised learning to semantically segment 2D images. The model works by using self-distillation, a method that uses teacher and student networks. In the forward pass, the teacher and student networks receive two different crops of the image as an input. The output of the teacher network is centered, and both the teacher and student network outputs are passed through softmax and loss functions. During backpropagation, only the student's weights are updated. The teacher's weights are updated by applying an exponential moving average on the student's weights.
                </p> 
                <p> 
                  <a href="https://arxiv.org/abs/2201.03546">LSeg (Language-Driven Semantic Segmentation)</a> is a zero-shot semantic image segmentation model. Traditional semantic segmentation models require labeled training data which is usually created by human annotators. This process is both time and labor intensive, and it is also prone to human-errors. Some zero-shot segmentation models use standard word embeddings while focusing on the image encoder. LSeg improves upon zero-shot semantic segmentation by using the CLIP model to train an image encoder to produce input image embeddings that are similar to the corresponding label embeddings produced by the text encoder.
                </p>
                <p>
                  Although NeRF renderings are used in some of the mentioned papers, it is time-consuming to train and can take up to several days to model a single scene. To mitigate this, <a href="https://arxiv.org/abs/2203.09517">TensoRF</a> decomposes 4D radiance field tensors into multiple low-rank tensors to enable faster reconstruction and higher quality renderings than NeRF.
                </p>
                <p>
                  Work on rendering transparent objects is limited. <a href="https://arxiv.org/abs/2110.14217">Dex-NeRF</a>  uses NeRF to render the geometry of transparent objects to improve robot manipulation. The authors of Dex-NeRF have also created a publicly available dataset of real and synthetic transparent objects in real-world settings. While Dex-NeRF improves rendering scenes with transparent objects, it does not enable editing of scenes containing transparent objects.
                </p>
            </div>
        </div>
    </div>
    <!--/ Related work. -->

    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
            <h2 class="title is-3">Method</h2>
            <img src="static/images/feature_field.png" alt="Feature and Radiance Fields">
            <p>
              
              <span class="bolded">Feature and Radiance Fields</span>
            </p>
            <br>
            <img src="static/images/2d_scene_decomposition.png" alt="2D Scene Decomposition">
            <p>
              <span class="bolded">2D Scene Decomposition</span>
            </p>
            <br>
            <img src="static/images/3d_scene_decomposition.png" alt="3D Scene Decomposition">
            <p>
              <span class="bolded">3D Scene Decomposition</span>

            </p>
        </div>
    </div>
    <!--/ Method. -->

    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
            <h2 class="title is-3">Results</h2>
            <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                    frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </div>
        </div>
    </div>
    <!--/ Results. -->

  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://homes.cs.washington.edu/~kpar/nerfies/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
